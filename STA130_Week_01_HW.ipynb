{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21895b25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "row_n           0\n",
       "id              1\n",
       "name            0\n",
       "gender          0\n",
       "species         0\n",
       "birthday        0\n",
       "personality     0\n",
       "song           11\n",
       "phrase          0\n",
       "full_id         0\n",
       "url             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1\n",
    "\n",
    "import pandas as pd\n",
    "url = \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-05-05/villagers.csv\"\n",
    "df = pd.read_csv(url)\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bac8595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset has 391 rows and 11 columns.\n"
     ]
    }
   ],
   "source": [
    "# 2.1\n",
    "\n",
    "import pandas as pd\n",
    "url = \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-05-05/villagers.csv\"\n",
    "df = pd.read_csv(url)\n",
    "df.isna().sum()\n",
    "rows, columns = df.shape\n",
    "print(f\"The dataset has {rows} rows and {columns} columns.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712b847d",
   "metadata": {},
   "source": [
    "#2.2\n",
    "\n",
    "Generally, observations are the individual records in a dataset. In my dataset, \"villagers\" are my observations, which are the rows in my dataset.\n",
    "Variables are the characteristics of observations and represent multiple types of information about the observations. For example, \"name,\"\"species,\"\"personality\" and every other column are the variables in my dataset because they are describing my observations -- villagers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98440dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names in the dataset:\n",
      "Index(['row_n', 'id', 'name', 'gender', 'species', 'birthday', 'personality', 'song', 'phrase', 'full_id', 'url'], dtype='object')\n",
      "            row_n\n",
      "count  391.000000\n",
      "mean   239.902813\n",
      "std    140.702672\n",
      "min      2.000000\n",
      "25%    117.500000\n",
      "50%    240.000000\n",
      "75%    363.500000\n",
      "max    483.000000\n",
      "species\n",
      "cat          23\n",
      "rabbit       20\n",
      "frog         18\n",
      "squirrel     18\n",
      "duck         17\n",
      "dog          16\n",
      "cub          16\n",
      "pig          15\n",
      "bear         15\n",
      "mouse        15\n",
      "horse        15\n",
      "bird         13\n",
      "penguin      13\n",
      "sheep        13\n",
      "elephant     11\n",
      "wolf         11\n",
      "ostrich      10\n",
      "deer         10\n",
      "eagle         9\n",
      "gorilla       9\n",
      "chicken       9\n",
      "koala         9\n",
      "goat          8\n",
      "hamster       8\n",
      "kangaroo      8\n",
      "monkey        8\n",
      "anteater      7\n",
      "hippo         7\n",
      "tiger         7\n",
      "alligator     7\n",
      "lion          7\n",
      "bull          6\n",
      "rhino         6\n",
      "cow           4\n",
      "octopus       3\n",
      "Name: count, dtype: int64\n",
      "personality\n",
      "lazy      60\n",
      "normal    59\n",
      "cranky    55\n",
      "snooty    55\n",
      "jock      55\n",
      "peppy     49\n",
      "smug      34\n",
      "uchi      24\n",
      "Name: count, dtype: int64\n",
      "   row_n       id  ...           full_id                                                url\n",
      "0      2  admiral  ...  villager-admiral  https://villagerdb.com/images/villagers/thumb/...\n",
      "1      3  agent-s  ...  villager-agent-s  https://villagerdb.com/images/villagers/thumb/...\n",
      "2      4    agnes  ...    villager-agnes  https://villagerdb.com/images/villagers/thumb/...\n",
      "3      6       al  ...       villager-al  https://villagerdb.com/images/villagers/thumb/...\n",
      "4      7  alfonso  ...  villager-alfonso  https://villagerdb.com/images/villagers/thumb/...\n",
      "\n",
      "[5 rows x 11 columns]\n",
      "       name    species personality\n",
      "0   Admiral       bird      cranky\n",
      "1   Agent S   squirrel       peppy\n",
      "2     Agnes        pig        uchi\n",
      "3        Al    gorilla        lazy\n",
      "4   Alfonso  alligator        lazy\n",
      "5     Alice      koala      normal\n",
      "6      Alli  alligator      snooty\n",
      "7    Amelia      eagle      snooty\n",
      "8  Anabelle   anteater       peppy\n",
      "9   Anchovy       bird        lazy\n"
     ]
    }
   ],
   "source": [
    "# 3\n",
    "\n",
    "# Simple way to output a summary of this dataset\n",
    "    pd.set_option('display.max_columns', 5)\n",
    "    pd.set_option('display.width', 1000)\n",
    "\n",
    "    print(df.head())\n",
    "    print(df[['name', 'species', 'personality']].head(10))\n",
    "else:\n",
    "    print(\"The columns 'species' and/or 'personality' do not exist in the dataset.\")\n",
    "\n",
    "# Examples of using df.describe() and df['column'].value_counts()\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-05-05/villagers.csv'\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "print(\"Column names in the dataset:\")\n",
    "print(df.columns)\n",
    "\n",
    "if 'species' in df.columns and 'personality' in df.columns:\n",
    "    numeric_summary = df.describe()\n",
    "    print(numeric_summary)\n",
    "\n",
    "    species_counts = df['species'].value_counts()\n",
    "    print(species_counts)\n",
    "\n",
    "    personality_counts = df['personality'].value_counts()\n",
    "    print(personality_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ccb02d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (891, 15)\n",
      "\n",
      "Description of the dataset (numeric columns only):\n",
      "         survived      pclass         age       sibsp       parch        fare\n",
      "count  891.000000  891.000000  714.000000  891.000000  891.000000  891.000000\n",
      "mean     0.383838    2.308642   29.699118    0.523008    0.381594   32.204208\n",
      "std      0.486592    0.836071   14.526497    1.102743    0.806057   49.693429\n",
      "min      0.000000    1.000000    0.420000    0.000000    0.000000    0.000000\n",
      "25%      0.000000    2.000000   20.125000    0.000000    0.000000    7.910400\n",
      "50%      0.000000    3.000000   28.000000    0.000000    0.000000   14.454200\n",
      "75%      1.000000    3.000000   38.000000    1.000000    0.000000   31.000000\n",
      "max      1.000000    3.000000   80.000000    8.000000    6.000000  512.329200\n",
      "\n",
      "Non-numeric columns:\n",
      "['sex', 'embarked', 'class', 'who', 'adult_male', 'deck', 'embark_town', 'alive', 'alone']\n",
      "\n",
      "Missing values in numeric columns:\n",
      "survived      0\n",
      "pclass        0\n",
      "age         177\n",
      "sibsp         0\n",
      "parch         0\n",
      "fare          0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 4\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the Titanic dataset\n",
    "url = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Get the shape of the dataset\n",
    "df_shape = df.shape\n",
    "\n",
    "# Get the description of the dataset\n",
    "df_description = df.describe()\n",
    "\n",
    "# Checking for non-numeric columns\n",
    "non_numeric_columns = df.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "\n",
    "# Checking for missing values in numeric columns\n",
    "missing_values_in_numeric = df.select_dtypes(include=[np.number]).isnull().sum()\n",
    "\n",
    "# Output the results\n",
    "print(f\"Dataset shape: {df_shape}\")\n",
    "print(\"\\nDescription of the dataset (numeric columns only):\")\n",
    "print(df_description)\n",
    "print(\"\\nNon-numeric columns:\")\n",
    "print(non_numeric_columns)\n",
    "print(\"\\nMissing values in numeric columns:\")\n",
    "print(missing_values_in_numeric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d352aa",
   "metadata": {},
   "source": [
    "#4\n",
    "\n",
    "When you have non-numeric variables, df.shape would count the non-numeric variables as the same as the numerica variables, but df.describe() would only count the numeric variables unless you code df.describe(include='all'). In summarization, df.describe() creates different statistics for non-numeric columns because it counts less columns when a dataset has non-numeric columns.\n",
    "When you have missing values in a dataset, df.describe() will not count the row that has a missing value in a numeric column, unless you code df.describe(include='all'). Differently, df.shape will count the total number of rows and columns as (rows, columns) whether a column has a missing value or not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2054d185",
   "metadata": {},
   "source": [
    "#5\n",
    "\n",
    "The biggest difference between an attribute and a method is what they are representing. An attribute represents data, so it is a direct access to the stored data and does not require computation when we code and access an attribute. For example, df.shape allows us to get the shape of DataFrame directly because the number of rows and columns is already stored and known. Differently, a method requires computation and executing codes because it is a function that operates on the data within the object. Consequently, a method requires parentheses because they need to take argument to operate computations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d36863",
   "metadata": {},
   "source": [
    "###Summary of Chat Session\n",
    "\n",
    "In this session, we discussed various aspects of working with datasets using pandas in Python, focusing on methods for summarizing and understanding data. Here is a breakdown of the key points covered:\n",
    "\n",
    "1. **Dataset Loading and Inspection**:\n",
    "   - We used two datasets hosted on GitHub: the villagers dataset from the URL `https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-05-05/villagers.csv` and the Titanic dataset from `https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv`.\n",
    "   - The datasets were loaded into a pandas DataFrame using `pd.read_csv()`.\n",
    "\n",
    "2. **Attributes vs. Methods**:\n",
    "   - **Attributes**: Properties of the DataFrame (e.g., `df.shape`) that do not require parentheses. They directly provide information about the DataFrame, such as its dimensions.\n",
    "   - **Methods**: Functions associated with the DataFrame (e.g., `df.describe()`) that perform actions or computations. They require parentheses because they can take arguments and perform operations on the data.\n",
    "\n",
    "3. **Using `df.describe()`**:\n",
    "   - The `df.describe()` method provides summary statistics for numeric columns, including:\n",
    "     - **count**: Number of non-missing values in the column.\n",
    "     - **mean**: Average of the values.\n",
    "     - **std**: Standard deviation, indicating the spread of values.\n",
    "     - **min, 25%, 50%, 75%, max**: Minimum, quartiles, and maximum values of the column.\n",
    "   - Missing values (`NaN`) are ignored in these calculations, which means `df.describe()` computes statistics based only on the non-missing values.\n",
    "\n",
    "4. **Using `df.shape`**:\n",
    "   - The `df.shape` attribute provides a tuple representing the dimensions of the DataFrame (number of rows and columns).\n",
    "   - Unlike `df.describe()`, it is not affected by missing values or the data types of the columns.\n",
    "\n",
    "5. **Working with Large Datasets**:\n",
    "   - To handle large datasets more effectively, we discussed using `df.head()` to preview the first few rows.\n",
    "   - Adjusting pandas display options using `pd.set_option()` helps customize the output, making it more readable when working with wide DataFrames.\n",
    "\n",
    "6. **Analyzing Categorical Data**:\n",
    "   - The `df['column'].value_counts()` method was used to analyze the distribution of values in categorical columns like `species` and `personality` in the villagers dataset.\n",
    "\n",
    "7. **Handling Missing Data**:\n",
    "   - When using methods like `df.describe()`, missing data (`NaN`) is automatically excluded from calculations.\n",
    "   - The `count` value provided by `df.describe()` indicates how many non-missing values were used for the calculations in each column.\n",
    "\n",
    "###Key Python Code Examples\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# Load the villagers dataset\n",
    "url = \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-05-05/villagers.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Display the shape of the dataset\n",
    "print(df.shape)\n",
    "\n",
    "# Get summary statistics for numeric columns\n",
    "print(df.describe())\n",
    "\n",
    "# Value counts for categorical columns\n",
    "print(df['species'].value_counts())\n",
    "print(df['personality'].value_counts())\n",
    "\n",
    "# Adjust display options for better readability\n",
    "pd.set_option('display.max_columns', 5)\n",
    "pd.set_option('display.width', 1000)\n",
    "print(df.head())\n",
    "```\n",
    "\n",
    "###Summary of Concepts\n",
    "- **Attributes** (`df.shape`) provide direct information about the DataFrame.\n",
    "- **Methods** (`df.describe()`) perform operations on the DataFrame and often require parentheses.\n",
    "- **Missing Values** are ignored by methods like `df.describe()`, affecting statistics like `mean` and `count`.\n",
    "- **Categorical Data Analysis** can be done using `df['column'].value_counts()` to understand the distribution of values.\n",
    "\n",
    "This summary provides a reference for understanding and summarizing data in pandas, especially when dealing with large datasets and missing values.\n",
    "\n",
    "###Link to the chat session\n",
    "https://chatgpt.com/share/66eb6731-4a14-8001-9f8b-f6335c69f156"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf57bd48",
   "metadata": {},
   "source": [
    "#6\n",
    "\n",
    "1.\n",
    "Definition of each summary statistics df.describe() method gives.\n",
    "count: the number of non-missing values in each column.\n",
    "mean: the average of values in each column, which is the sum of all values divided by the number of values.\n",
    "std: stands for standard deviation, which represents the spread of values around the mean.\n",
    "min: represents the minimum value of the column.\n",
    "25%: represents the first quartile, which is the value before 25% of the data falls.\n",
    "50%: represents the median value of the column, which is the middle value of the data falls.\n",
    "75%: represents the third quartile, which is the value before 75% of the data falls.\n",
    "max: represents the maximum value of the column.\n",
    "\n",
    "2.\n",
    "These can only be calculated for numeric variables because those numeric quantities have to be numbers to be calculated, categorical variables are not able to calculated to these statistics quantities.\n",
    "\n",
    "3.\n",
    "Since df.describe() only describes the non missing values, so it doesnâ€™t affect the result whether we delete the missing values or not \n",
    "4.\n",
    "It is clear that df.describe() function ignores missing values in calculation, which consequently affects the count because the count is the number of non-missing value so the count would be less when there is a missing value, so we can remove columns of missing values. But when the missing values are significant, we need to keep them for our analysis because they are still useful. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d75d17",
   "metadata": {},
   "source": [
    "#7.1\n",
    "\n",
    "Using df.dropna() might be preferred over using del df['col'] when we only want to delete the row which has missing value. For example, you has a huge DataFrame that has 10,000 observations of a type of plant's growth record and you're using the DataFrame to study about this type of plant's growthing activity, you have temperature as one of the variables in growth record, but 2 in 10,000 plant do not have a value in the temperature column, now you do not want to delete the whole column of temperature variable because it is still crutial and useful for your study. In this case, you want to use df.dropna() to just delete the two rows that are missing a temperature value.\n",
    "\n",
    "#7.2\n",
    "\n",
    "Using del df['col'] might be preferred over df.dropna() when we want to remove the whole column that has a significant amount of missing data. For example, in your study of a plant's growth activity, you have 10,000 observations, and 9,999 of them are missing a value for \"size of seed\" variable, and now you want to delete the whole column of \"size of seed\" variable because it is no longer useful for your study. In this case, it is more efficient to use del df['col'] to delete the whole column.\n",
    "\n",
    "#7.3\n",
    "\n",
    "In the interest of efficiency, applying del df['col'] before df.dropna() when both are used could be important. We are reducing the data size when we are using del df['col'] and df.dropna(), which means that we would want to delete the columns that are missing a lot of values first, and it would make row reducing easier and less. Another important reason to use df['col'] first is that we want to ensure the data cleaning process is accurate and prevent accidental data loss. df['col'] first eliminate the data that are not useful to the user, like what we did in the previous question, so that df.dropna() is less likely to accidently delete data that are relevant and useful for the user. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c74f5f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before and After Cleaning Report:\n",
      "Initial Shape: (891, 15)\n",
      "Columns with Missing Data (Initial):\n",
      "age            177\n",
      "embarked         2\n",
      "deck           688\n",
      "embark_town      2\n",
      "dtype: int64\n",
      "Final Shape After Cleaning: (712, 14)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv'\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "initial_shape = df.shape\n",
    "\n",
    "# Identify columns with missing data because we want to make the cleaning process efficient and accurate\n",
    "missing_data_columns = df.isnull().sum()\n",
    "\n",
    "# Remove a column with significant missing values, 'deck', using del\n",
    "del df['deck']\n",
    "\n",
    "# Use dropna() to remove remaining rows with any missing data\n",
    "df_cleaned = df.dropna()\n",
    "\n",
    "# Report the shape after cleaning\n",
    "final_shape = df_cleaned.shape\n",
    "\n",
    "# Compile \"before and after\" report\n",
    "print(\"Before and After Cleaning Report:\")\n",
    "print(f\"Initial Shape: {initial_shape}\")\n",
    "print(\"Columns with Missing Data (Initial):\")\n",
    "print(missing_data_columns[missing_data_columns > 0])\n",
    "print(f\"Final Shape After Cleaning: {final_shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01c97d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns analyzed: 6\n",
      "\n",
      "Values in the 'count' row:\n",
      " survived    891.0\n",
      "pclass      891.0\n",
      "age         714.0\n",
      "sibsp       891.0\n",
      "parch       891.0\n",
      "fare        891.0\n",
      "Name: count, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv\"\n",
    "titanic_df = pd.read_csv(url)\n",
    "\n",
    "titanic_description = titanic_df.describe()\n",
    "num_columns_analyzed = titanic_description.shape[1]\n",
    "count_values = titanic_description.loc['count']\n",
    "print(\"Number of columns analyzed:\", num_columns_analyzed)\n",
    "print(\"\\nValues in the 'count' row:\\n\", count_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fc7cee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall description (count):\n",
      "age     714.0\n",
      "fare    891.0\n",
      "Name: count, dtype: float64\n",
      "\n",
      "Grouped description (count) by 'pclass':\n",
      "pclass\n",
      "1    186.0\n",
      "2    173.0\n",
      "3    355.0\n",
      "Name: count, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# 8.1\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv\"\n",
    "titanic_df = pd.read_csv(url)\n",
    "\n",
    "overall_description = titanic_df.describe()\n",
    "\n",
    "grouped_description = titanic_df.groupby('pclass')['age'].describe()\n",
    "\n",
    "print(\"Overall description (count):\")\n",
    "print(overall_description.loc['count', ['age', 'fare']])\n",
    "\n",
    "print(\"\\nGrouped description (count) by 'pclass':\")\n",
    "print(grouped_description['count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b127c79",
   "metadata": {},
   "source": [
    "Another example is that we can also group the Titanic passengers by their boarding location (\"embarked\") and provides detailed statistics for the \"age\" column within each group. Then we can use .describe() function to include statistics like count, mean, and median age, allowing us to compare age distributions of passengers who boarded at different ports."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66dd178e",
   "metadata": {},
   "source": [
    "#8.2\n",
    "\n",
    "df.describe() function gives a global summary of each column's completeness, which ignores categories or subgroups. Differently, df.groupby(\"col1\")[\"col2\"].describe() breaks this down by providing a detailed view of how \"col2\" is distributed within each category of \"col1\". This method highlights variations in data completeness and distribution at a more granular level, showing where missing values may be concentrated within specific groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f252f8c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#8.3.A.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 4\u001b[0m titanic_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(url)\n\u001b[1;32m      6\u001b[0m titanic_description \u001b[38;5;241m=\u001b[39m titanic_df\u001b[38;5;241m.\u001b[39mdescribe()\n\u001b[1;32m      7\u001b[0m num_columns_analyzed \u001b[38;5;241m=\u001b[39m titanic_description\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# 8.3.\n",
    "\n",
    "# ChatBot is more efficient than google when I want to do debugging. I found this is basically because ChatBot is\n",
    "#    more personalized. ChatBot Understands the specific error that is encountered with a specific code that needs\n",
    "#    to be debugged, and it gives back a personalized specific solution. Differently with google, it often costs\n",
    "#    me more time to research for the TypeError, and it is time-consuming to absorb and learn about these TypeError\n",
    "#    without a model like ChatBot's help.\n",
    "\n",
    "# A. before\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv\"\n",
    "titanic_df = pd.read_csv(url)\n",
    "\n",
    "titanic_description = titanic_df.describe()\n",
    "num_columns_analyzed = titanic_description.shape[1]\n",
    "count_values = titanic_description.loc['count']\n",
    "print(\"Number of columns analyzed:\", num_columns_analyzed)\n",
    "print(\"\\nValues in the 'count' row:\\n\", count_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9971ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns analyzed: 6\n",
      "\n",
      "Values in the 'count' row:\n",
      " survived    891.0\n",
      "pclass      891.0\n",
      "age         714.0\n",
      "sibsp       891.0\n",
      "parch       891.0\n",
      "fare        891.0\n",
      "Name: count, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# A. after\n",
    "\n",
    "import pandas as pd  # Import pandas\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv\"\n",
    "titanic_df = pd.read_csv(url)\n",
    "\n",
    "titanic_description = titanic_df.describe()\n",
    "num_columns_analyzed = titanic_description.shape[1]\n",
    "count_values = titanic_description.loc['count']\n",
    "\n",
    "print(\"Number of columns analyzed:\", num_columns_analyzed)\n",
    "print(\"\\nValues in the 'count' row:\\n\", count_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64ee3855",
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 404: Not Found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      6\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanics.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 7\u001b[0m titanic_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m titanic_description \u001b[38;5;241m=\u001b[39m titanic_df\u001b[38;5;241m.\u001b[39mdescribe()\n\u001b[1;32m     10\u001b[0m num_columns_analyzed \u001b[38;5;241m=\u001b[39m titanic_description\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    945\u001b[0m )\n\u001b[1;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1704\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1706\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1711\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1712\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1714\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1715\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1716\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/common.py:718\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    715\u001b[0m     codecs\u001b[38;5;241m.\u001b[39mlookup_error(errors)\n\u001b[1;32m    717\u001b[0m \u001b[38;5;66;03m# open URLs\u001b[39;00m\n\u001b[0;32m--> 718\u001b[0m ioargs \u001b[38;5;241m=\u001b[39m \u001b[43m_get_filepath_or_buffer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    719\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    720\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    722\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    724\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    726\u001b[0m handle \u001b[38;5;241m=\u001b[39m ioargs\u001b[38;5;241m.\u001b[39mfilepath_or_buffer\n\u001b[1;32m    727\u001b[0m handles: \u001b[38;5;28mlist\u001b[39m[BaseBuffer]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/common.py:372\u001b[0m, in \u001b[0;36m_get_filepath_or_buffer\u001b[0;34m(filepath_or_buffer, encoding, compression, mode, storage_options)\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;66;03m# assuming storage_options is to be interpreted as headers\u001b[39;00m\n\u001b[1;32m    371\u001b[0m req_info \u001b[38;5;241m=\u001b[39m urllib\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39mRequest(filepath_or_buffer, headers\u001b[38;5;241m=\u001b[39mstorage_options)\n\u001b[0;32m--> 372\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq_info\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m req:\n\u001b[1;32m    373\u001b[0m     content_encoding \u001b[38;5;241m=\u001b[39m req\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Encoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m content_encoding \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgzip\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    375\u001b[0m         \u001b[38;5;66;03m# Override compression based on Content-Encoding header\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/common.py:274\u001b[0m, in \u001b[0;36murlopen\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;124;03mLazy-import wrapper for stdlib urlopen, as that imports a big chunk of\u001b[39;00m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;124;03mthe stdlib.\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01murllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrequest\u001b[39;00m\n\u001b[0;32m--> 274\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43murllib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/urllib/request.py:216\u001b[0m, in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    215\u001b[0m     opener \u001b[38;5;241m=\u001b[39m _opener\n\u001b[0;32m--> 216\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopener\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/urllib/request.py:525\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m processor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_response\u001b[38;5;241m.\u001b[39mget(protocol, []):\n\u001b[1;32m    524\u001b[0m     meth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(processor, meth_name)\n\u001b[0;32m--> 525\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/urllib/request.py:634\u001b[0m, in \u001b[0;36mHTTPErrorProcessor.http_response\u001b[0;34m(self, request, response)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;66;03m# According to RFC 2616, \"2xx\" code indicates that the client's\u001b[39;00m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;66;03m# request was successfully received, understood, and accepted.\u001b[39;00m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m):\n\u001b[0;32m--> 634\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhdrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/urllib/request.py:563\u001b[0m, in \u001b[0;36mOpenerDirector.error\u001b[0;34m(self, proto, *args)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_err:\n\u001b[1;32m    562\u001b[0m     args \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mdict\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp_error_default\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m orig_args\n\u001b[0;32m--> 563\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/urllib/request.py:496\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[1;32m    495\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[0;32m--> 496\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    497\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    498\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/urllib/request.py:643\u001b[0m, in \u001b[0;36mHTTPDefaultErrorHandler.http_error_default\u001b[0;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[1;32m    642\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhttp_error_default\u001b[39m(\u001b[38;5;28mself\u001b[39m, req, fp, code, msg, hdrs):\n\u001b[0;32m--> 643\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(req\u001b[38;5;241m.\u001b[39mfull_url, code, msg, hdrs, fp)\n",
      "\u001b[0;31mHTTPError\u001b[0m: HTTP Error 404: Not Found"
     ]
    }
   ],
   "source": [
    "# B. before\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanics.csv\"\n",
    "titanic_df = pd.read_csv(url)\n",
    "\n",
    "titanic_description = titanic_df.describe()\n",
    "num_columns_analyzed = titanic_description.shape[1]\n",
    "count_values = titanic_description.loc['count']\n",
    "print(\"Number of columns analyzed:\", num_columns_analyzed)\n",
    "print(\"\\nValues in the 'count' row:\\n\", count_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d20d5c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns analyzed: 6\n",
      "\n",
      "Values in the 'count' row:\n",
      " survived    891.0\n",
      "pclass      891.0\n",
      "age         714.0\n",
      "sibsp       891.0\n",
      "parch       891.0\n",
      "fare        891.0\n",
      "Name: count, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# B. after\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv\"\n",
    "titanic_df = pd.read_csv(url)\n",
    "\n",
    "titanic_description = titanic_df.describe()\n",
    "num_columns_analyzed = titanic_description.shape[1]\n",
    "count_values = titanic_description.loc['count']\n",
    "\n",
    "print(\"Number of columns analyzed:\", num_columns_analyzed)\n",
    "print(\"\\nValues in the 'count' row:\\n\", count_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f2e735f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'titanic_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m titanic_Df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(url)\n\u001b[0;32m----> 8\u001b[0m titanic_description \u001b[38;5;241m=\u001b[39m \u001b[43mtitanic_df\u001b[49m\u001b[38;5;241m.\u001b[39mdescribe()\n\u001b[1;32m      9\u001b[0m num_columns_analyzed \u001b[38;5;241m=\u001b[39m titanic_description\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     10\u001b[0m count_values \u001b[38;5;241m=\u001b[39m titanic_description\u001b[38;5;241m.\u001b[39mloc[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcount\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'titanic_df' is not defined"
     ]
    }
   ],
   "source": [
    "# C. before\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv\"\n",
    "titanic_Df = pd.read_csv(url)\n",
    "\n",
    "titanic_description = titanic_df.describe()\n",
    "num_columns_analyzed = titanic_description.shape[1]\n",
    "count_values = titanic_description.loc['count']\n",
    "print(\"Number of columns analyzed:\", num_columns_analyzed)\n",
    "print(\"\\nValues in the 'count' row:\\n\", count_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c281647c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns analyzed: 6\n",
      "\n",
      "Values in the 'count' row:\n",
      " survived    891.0\n",
      "pclass      891.0\n",
      "age         714.0\n",
      "sibsp       891.0\n",
      "parch       891.0\n",
      "fare        891.0\n",
      "Name: count, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# C. after\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv\"\n",
    "titanic_df = pd.read_csv(url)  # Use 'titanic_df' with a lowercase 'd'\n",
    "\n",
    "titanic_description = titanic_df.describe()\n",
    "num_columns_analyzed = titanic_description.shape[1]\n",
    "count_values = titanic_description.loc['count']\n",
    "\n",
    "print(\"Number of columns analyzed:\", num_columns_analyzed)\n",
    "print(\"\\nValues in the 'count' row:\\n\", count_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e946c70",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'(' was never closed (2475561347.py, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 6\u001b[0;36m\u001b[0m\n\u001b[0;31m    titanic_df = pd.read_csv(url\u001b[0m\n\u001b[0m                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m '(' was never closed\n"
     ]
    }
   ],
   "source": [
    "# D. before\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv\"\n",
    "titanic_df = pd.read_csv(url\n",
    "\n",
    "titanic_description = titanic_df.describe()\n",
    "num_columns_analyzed = titanic_description.shape[1]\n",
    "count_values = titanic_description.loc['count']\n",
    "print(\"Number of columns analyzed:\", num_columns_analyzed)\n",
    "print(\"\\nValues in the 'count' row:\\n\", count_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5cbdae5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns analyzed: 6\n",
      "\n",
      "Values in the 'count' row:\n",
      " survived    891.0\n",
      "pclass      891.0\n",
      "age         714.0\n",
      "sibsp       891.0\n",
      "parch       891.0\n",
      "fare        891.0\n",
      "Name: count, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# D. after\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv\"\n",
    "titanic_df = pd.read_csv(url)  # Closed parenthesis here\n",
    "\n",
    "titanic_description = titanic_df.describe()\n",
    "num_columns_analyzed = titanic_description.shape[1]\n",
    "count_values = titanic_description.loc['count']\n",
    "\n",
    "print(\"Number of columns analyzed:\", num_columns_analyzed)\n",
    "print(\"\\nValues in the 'count' row:\\n\", count_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16a5564d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'titanic_description_shape' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m titanic_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(url)\n\u001b[1;32m      8\u001b[0m titanic_description \u001b[38;5;241m=\u001b[39m titanic_df\u001b[38;5;241m.\u001b[39mdescribe()\n\u001b[0;32m----> 9\u001b[0m num_columns_analyzed \u001b[38;5;241m=\u001b[39m \u001b[43mtitanic_description_shape\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     10\u001b[0m count_values \u001b[38;5;241m=\u001b[39m titanic_description\u001b[38;5;241m.\u001b[39mloc[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcount\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of columns analyzed:\u001b[39m\u001b[38;5;124m\"\u001b[39m, num_columns_analyzed)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'titanic_description_shape' is not defined"
     ]
    }
   ],
   "source": [
    "# E. before\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv\"\n",
    "titanic_df = pd.read_csv(url)\n",
    "\n",
    "titanic_description = titanic_df.describe()\n",
    "num_columns_analyzed = titanic_description_shape[1]\n",
    "count_values = titanic_description.loc['count']\n",
    "print(\"Number of columns analyzed:\", num_columns_analyzed)\n",
    "print(\"\\nValues in the 'count' row:\\n\", count_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08944247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns analyzed: 6\n",
      "\n",
      "Values in the 'count' row:\n",
      " survived    891.0\n",
      "pclass      891.0\n",
      "age         714.0\n",
      "sibsp       891.0\n",
      "parch       891.0\n",
      "fare        891.0\n",
      "Name: count, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# E. after\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv\"\n",
    "titanic_df = pd.read_csv(url)\n",
    "\n",
    "titanic_description = titanic_df.describe()\n",
    "num_columns_analyzed = titanic_description.shape[1]  # Corrected this line\n",
    "count_values = titanic_description.loc['count']\n",
    "\n",
    "print(\"Number of columns analyzed:\", num_columns_analyzed)\n",
    "print(\"\\nValues in the 'count' row:\\n\", count_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c526f93",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'haircolor'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/indexes/base.py:3790\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3789\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3790\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3791\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:152\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:181\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'haircolor'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m titanic_description \u001b[38;5;241m=\u001b[39m titanic_df\u001b[38;5;241m.\u001b[39mdescribe()\n\u001b[1;32m      9\u001b[0m num_columns_analyzed \u001b[38;5;241m=\u001b[39m titanic_description\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m---> 10\u001b[0m count_values \u001b[38;5;241m=\u001b[39m \u001b[43mtitanic_description\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhaircolor\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of columns analyzed:\u001b[39m\u001b[38;5;124m\"\u001b[39m, num_columns_analyzed)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mValues in the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcount\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m row:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, count_values)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/indexing.py:1153\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1150\u001b[0m axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   1152\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj)\n\u001b[0;32m-> 1153\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaybe_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/indexing.py:1393\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1391\u001b[0m \u001b[38;5;66;03m# fall thru to straight lookup\u001b[39;00m\n\u001b[1;32m   1392\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_key(key, axis)\n\u001b[0;32m-> 1393\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_label\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/indexing.py:1343\u001b[0m, in \u001b[0;36m_LocIndexer._get_label\u001b[0;34m(self, label, axis)\u001b[0m\n\u001b[1;32m   1341\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_label\u001b[39m(\u001b[38;5;28mself\u001b[39m, label, axis: AxisInt):\n\u001b[1;32m   1342\u001b[0m     \u001b[38;5;66;03m# GH#5567 this will fail if the label is not present in the axis.\u001b[39;00m\n\u001b[0;32m-> 1343\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mxs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/generic.py:4236\u001b[0m, in \u001b[0;36mNDFrame.xs\u001b[0;34m(self, key, axis, level, drop_level)\u001b[0m\n\u001b[1;32m   4234\u001b[0m             new_index \u001b[38;5;241m=\u001b[39m index[loc]\n\u001b[1;32m   4235\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 4236\u001b[0m     loc \u001b[38;5;241m=\u001b[39m \u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4238\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(loc, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[1;32m   4239\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m loc\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mbool_:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/indexes/base.py:3797\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3792\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3793\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3794\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3795\u001b[0m     ):\n\u001b[1;32m   3796\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3797\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3798\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3799\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3800\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3801\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3802\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'haircolor'"
     ]
    }
   ],
   "source": [
    "# F. before\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv\"\n",
    "titanic_df = pd.read_csv(url)\n",
    "\n",
    "titanic_description = titanic_df.describe()\n",
    "num_columns_analyzed = titanic_description.shape[1]\n",
    "count_values = titanic_description.loc['haircolor']\n",
    "print(\"Number of columns analyzed:\", num_columns_analyzed)\n",
    "print(\"\\nValues in the 'count' row:\\n\", count_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "839ebc21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns analyzed: 6\n",
      "\n",
      "Values in the 'count' row:\n",
      " survived    891.0\n",
      "pclass      891.0\n",
      "age         714.0\n",
      "sibsp       891.0\n",
      "parch       891.0\n",
      "fare        891.0\n",
      "Name: count, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# F. after\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv\"\n",
    "titanic_df = pd.read_csv(url)\n",
    "\n",
    "titanic_description = titanic_df.describe()\n",
    "num_columns_analyzed = titanic_description.shape[1]\n",
    "count_values = titanic_description.loc['count']  # Corrected to 'count'\n",
    "\n",
    "print(\"Number of columns analyzed:\", num_columns_analyzed)\n",
    "print(\"\\nValues in the 'count' row:\\n\", count_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01516cb0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'count' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m titanic_description \u001b[38;5;241m=\u001b[39m titanic_df\u001b[38;5;241m.\u001b[39mdescribe()\n\u001b[1;32m      9\u001b[0m num_columns_analyzed \u001b[38;5;241m=\u001b[39m titanic_description\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m---> 10\u001b[0m count_values \u001b[38;5;241m=\u001b[39m titanic_description\u001b[38;5;241m.\u001b[39mloc[\u001b[43mcount\u001b[49m]\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of columns analyzed:\u001b[39m\u001b[38;5;124m\"\u001b[39m, num_columns_analyzed)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mValues in the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcount\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m row:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, count_values)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'count' is not defined"
     ]
    }
   ],
   "source": [
    "# G. before\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv\"\n",
    "titanic_df = pd.read_csv(url)\n",
    "\n",
    "titanic_description = titanic_df.describe()\n",
    "num_columns_analyzed = titanic_description.shape[1]\n",
    "count_values = titanic_description.loc[count]\n",
    "print(\"Number of columns analyzed:\", num_columns_analyzed)\n",
    "print(\"\\nValues in the 'count' row:\\n\", count_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c640dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns analyzed: 6\n",
      "\n",
      "Values in the 'count' row:\n",
      " survived    891.0\n",
      "pclass      891.0\n",
      "age         714.0\n",
      "sibsp       891.0\n",
      "parch       891.0\n",
      "fare        891.0\n",
      "Name: count, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# G. after\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv\"\n",
    "titanic_df = pd.read_csv(url)\n",
    "\n",
    "titanic_description = titanic_df.describe()\n",
    "num_columns_analyzed = titanic_description.shape[1]\n",
    "count_values = titanic_description.loc['count']  # 'count' should be in quotes\n",
    "\n",
    "print(\"Number of columns analyzed:\", num_columns_analyzed)\n",
    "print(\"\\nValues in the 'count' row:\\n\", count_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fbbb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9\n",
    "Yes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694f477e",
   "metadata": {},
   "source": [
    "#2nd summary and link to it\n",
    "\n",
    "###Summary of Chat Session:\n",
    "\n",
    "1. **Data Cleaning with `df.dropna()` vs. `del df['col']`:**\n",
    "   - Discussed scenarios where using `df.dropna()` is preferred over `del df['col']`, focusing on row removal based on missing data in multiple columns.\n",
    "   - Explored situations where using `del df['col']` is preferred, particularly when a column has a high percentage of missing data and is not crucial for analysis.\n",
    "   - Demonstrated the importance of using `del df['col']` before `df.dropna()` to improve efficiency by reducing the dataset size and preventing unnecessary row drops.\n",
    "\n",
    "2. **Titanic Dataset Example:**\n",
    "   - Removed the column 'deck' (which had a high percentage of missing values) using `del df['deck']` and then used `df.dropna()` to remove rows with missing data in the remaining columns.\n",
    "   - Provided a \"before and after\" cleaning report to show the dataset's shape changes, highlighting the impact of this cleaning approach.\n",
    "   - Explained the importance of using `del df['col']` first to preserve relevant data and focus the cleaning process on more crucial columns.\n",
    "\n",
    "3. **Summarizing Data:**\n",
    "   - Provided code to generate descriptive statistics and counts of unique values for columns 'species' and 'personality' in a dataset.\n",
    "   - Discussed potential errors like `KeyError` when column names are not found in the DataFrame, providing troubleshooting steps (checking column names and ensuring correct dataset structure).\n",
    "\n",
    "4. **Error Handling and Debugging:**\n",
    "   - Encountered a `KeyError` when trying to access the 'species' column, indicating it didn't exist in the DataFrame.\n",
    "   - Suggested diagnostic steps, including printing column names and checking the dataset's structure, to resolve the issue.\n",
    "   - Provided a code snippet to conditionally proceed with analysis if the required columns exist.\n",
    "\n",
    "This session covered practical data cleaning techniques, error handling, and summarizing datasets, all relevant to data analysis and preprocessing tasks.\n",
    "\n",
    "https://chatgpt.com/share/66eb97bb-0e1c-8001-91e4-bf9bf48b6065"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbea72f5",
   "metadata": {},
   "source": [
    "#3rd summary and link to it\n",
    "\n",
    "###Summary of Titanic Dataset Analysis\n",
    "\n",
    "1. **Loading and Analyzing the Titanic Dataset**:\n",
    "   - We used the Titanic dataset available at `\"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv\"`.\n",
    "   - The dataset was loaded into a pandas DataFrame using `pd.read_csv(url)`.\n",
    "   - We used `df.describe()` to obtain summary statistics for the numerical columns in the dataset.\n",
    "\n",
    "2. **`df.describe()` Insights**:\n",
    "   - `df.describe()` provides a high-level summary of the dataset's numerical columns, including metrics like `count`, `mean`, `std`, `min`, `max`, etc.\n",
    "   - The \"count\" row in the output of `df.describe()` indicates the number of non-missing values for each column.\n",
    "   - The number of columns analyzed by `df.describe()` was determined using `titanic_description.shape[1]`.\n",
    "\n",
    "3. **Errors and Debugging**:\n",
    "   - We encountered several errors during the process:\n",
    "     - **`NameError: name 'pd' is not defined`**: This was due to missing the import statement for pandas. The issue was resolved by adding `import pandas as pd`.\n",
    "     - **`HTTPError: HTTP Error 404: Not Found`**: This occurred due to an incorrect URL for the dataset. We fixed it by using the correct URL.\n",
    "     - **Syntax and Typo Errors**: Missing parentheses and incorrect variable names caused issues. These were fixed by ensuring correct syntax and using consistent variable names.\n",
    "     - **`KeyError`**: Attempted to access a non-existent row ('haircolor') in the DataFrame. This was corrected by using the correct row label ('count').\n",
    "\n",
    "4. **Differences in Grouped Analysis**:\n",
    "   - Discussed the difference between `df.describe()` and `df.groupby(\"col1\")[\"col2\"].describe()`. The former provides a global overview of the dataset, while the latter gives a detailed view of specific subgroups defined by `col1`.\n",
    "\n",
    "This summary encapsulates our exploration of the Titanic dataset using pandas, including basic descriptive statistics, error handling, and an understanding of how data aggregation functions differ.\n",
    "\n",
    "https://chatgpt.com/share/66eb97e6-afcc-8001-a1bb-97b3cac3d7ad"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
